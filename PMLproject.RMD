---
title: "Predicting the form of an exercise"
author: "John Balo"
date: "July 25, 2017"
output:
  html_document: default
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Overview

More and more people are starting to use devices like the Jawbone Up, Nike FuelBand, and Fitbit to monitor their performance and health. This trend results in massive amounts of movement data, hiding numerous insights about exercise and fitness. The aim of this report is to predict whether an exercise was performed in the correct form based on data collected by personal fitness devices.

## The data

The collection of data was done with the use of accelerometers on the belt, forearm, arm and dumbell. The subjects of the research were 6 healthy young men between 20 and 28 years of age. They performed an barbell lifts in 5 different forms, of which one was correct and the rest incorrect.

The data for this project can be found here:

https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv

and

https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv

The data come from http://groupware.les.inf.puc-rio.br/har. A big thank you to the provider.

## Getting and cleaning the data

We will begin by loading the required libraries:

```{r echo = T, results = 'hide', message = F, warning=F}
library(caret)
library(plyr)
library(dplyr)
library(randomForest)
library(gbm)
```

Then we will download and store the data:

```{r echo = T, cache = T, message = F}
download.file("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv", "training.csv")
download.file("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv", "testing.csv")
training <- read.csv("training.csv")
testing <- read.csv("testing.csv")
dim(training); dim(testing)
```

Finally we will clean the data:

1. Removing NAs
```{r, echo = T, message = F}
bool <- is.na(training) #Creating a boolean NA/non-NA matrix
NAcount <- colSums(bool) #Calculating how many NAs each variable has
bool2 <- NAcount != 0 #Creating a boolean vector (true if there are NAs)
boolfr <- as.data.frame(bool2)
boolfr[,2] <- 1:160 #Adding an index
boolfr <- subset(boolfr, bool2 == TRUE, select = c(bool2, V2)) #Keeping only the indexes that correspond to NA variables
trainNA <- training[,-boolfr$V2] #Removing NAs from the datasets
testNA <- testing[,-boolfr$V2] #Removing NAs from the datasets
```

2. Removing factor variables except for the outcome
```{r, echo = T, message = F}
temp1 <- data.frame(class = unlist(lapply(X = trainNA, FUN = class)), index12 = 1:93) #Getting the class of each variable + adding an index
temp1 <- mutate(.data=temp1, factorbool = class == "factor")
rownames(temp1) <- names(trainNA)
temp2 <- subset(temp1, factorbool)
temp2 <- temp2[-c(2,37),]

trainclean <- trainNA[,-temp2$index12]
trainfinal <- trainclean[,-(1:7)]
testclean <- testNA[,-temp2$index12]
testfinal <- testclean[,-(1:7)] #Removing the descriptive variables in the beginning of the dataset
dim(trainfinal); dim(testfinal) 
```
We can see the reduction of variables in play.

## Training, Testing and Validation sets

We will split the data into three separate datasets in order to accurately calculate the out of sample error. The sets will be 75% training, 25% testing, 5% validation:

```{r, echo = T, message = F}
set.seed(2233)
inTrain <- createDataPartition(trainfinal$classe, p=0.7, list=FALSE)
newTraining <- trainfinal[inTrain, ]
temp3 <- trainfinal[-inTrain, ]
inTest <- createDataPartition(temp3$classe, p=0.75, list=FALSE)
newTesting <- temp3[inTest, ]
newValidation <- temp3[-inTest, ]
```

## Model selection

### Linear Discriminant Analysis:
First, we will try the lda model with no cross validation.
Aiming for an efficient and fairly accurate result in case the more demanding models fail due to lack of processing power.
```{r, echo = T, cache = T, results='hide', message = F}
modellda <- train(classe~., data = newTraining, method = "lda")
predictLDA <- predict(modellda, newdata=newTesting)
ldaMatrix <- confusionMatrix(predictLDA, newTesting$classe)

```

### Generalized Boosted Model:
We will next try the gbm model with cross validation: k = 5
```{r, echo = T, cache = T, results='hide', message = F}
modelgbm <- train(classe ~ ., data=newTraining, method = "gbm",
                trControl = trainControl(method = "repeatedcv", number = 5, repeats = 1),verbose = FALSE)
predictGBM <- predict(modelgbm, newdata=newTesting)
gbmMatrix <- confusionMatrix(predictGBM, newTesting$classe)

```

### Random Forests:
Finaly, we will try the rf model, again with cross validation: k = 5
```{r, echo = T, cache = T,results='hide', message = F}
modelrf <- randomForest(classe ~ ., data=newTraining, 
                trControl=trainControl(method="cv",number=5))
predictRF <- predict(modelrf, newdata=newTesting, type = "class")
rfMatrix <- confusionMatrix(predictRF, newTesting$classe)

```

### Comparing the Models:
```{r, echo = F}
"LDA results:"
ldaMatrix$overall[1:4]
ldaMatrix$table
"GBM results"
gbmMatrix$overall[1:4]
gbmMatrix$table
"RF results"
rfMatrix$overall[1:4]
rfMatrix$table
```
As we can see, the random forest method is substantialy more accurate than the other two models. We will use that one for our predictions.

## Computing the out-of-sample error

We will compute the out-of-sample error by using our model on the validation data. We will use the 1 - accuracy metric as the quantification of the error.

```{r, echo = F, message = F}
predictValid <- predict(modelrf, newValidation, type = "class")
validMatrix <- confusionMatrix(predictValid, newValidation$classe)
validMatrix$overall[1:4]
validMatrix$table
"Out-of-sample error:"
1-validMatrix$overall[1]
```
We see that our random forests model performs particularly well on the validation set. We are confident that the predictions on new data will be accurate.

## Predicting the Project Quiz

```{r, echo = T}
predict(modelrf, testfinal, type="class")
```
This prediction scored 100% on the quiz.

## Executive summary

The results of the report confirm that you can definetely predict the form of an executed exercise by using movement data from personal fitness devices. With enough data and an advanced machine learning model such as the random forests, we can acheive almost 100% accuracy in our predictions.